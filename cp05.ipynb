{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第5章：大規模言語モデル\n",
    "この章では、大規模言語モデル(LLM)の利用し、様々なタスクに取り組む。大規模言語モデルをプログラムからAPI経由で呼び出すことを想定しており、そのAPIの利用で費用が発生する可能性があることに留意せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40.Zero-Shot推論\n",
    "以下の問題の回答を作成せよ。ただし、解答生成はZero-Shot推論とせよ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "９世紀に活躍した人物に関係するできごとについて述べた次のア〜ウを年代の古い順に正しく並べよ。\n",
    "\n",
    "ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。\n",
    "イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。\n",
    "ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:18<00:00, 34.51s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正しい年代順は次のとおりです。  1. **イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。** （809年） 2. **ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。** （842年） 3. **ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。** （903年）    **解説:**  * イは嵯峨天皇の即位後すぐに起こった出来事です。 * ウは承和の変という事件の後、藤原良房が台頭した出来事です。 * アは菅原道真が追放された出来事で、時平が政界に台頭した後の出来事です。\n",
      "推論にかかった時間 : 18.266639232635498 [s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
    "    query = (\"9世紀に活躍した人物に関係するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\"\n",
    "             \"ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。\"\n",
    "             \"イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。\"\n",
    "             \"ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。\"\n",
    "             ) # ← ここをいろいろと変えて触ってみてください\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # 結果を表示\n",
    "    print(response.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"推論にかかった時間 : {str(end-start)} [s]\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41.Few-Shot推論\n",
    "以下の問題と回答を与え、問題40で示した質問の回答をfew-shot推論（この場合は4-shot推論）で生成せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本の近代化に関連するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\n",
    "\n",
    "ア　府知事・県令からなる地方官会議が設置された。\n",
    "イ　廃藩置県が実施され，中央から府知事・県令が派遣される体制になった。\n",
    "ウ　すべての藩主が，天皇に領地と領民を返還した。\n",
    "\n",
    "解答: ウ→イ→ア"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "江戸幕府の北方での対外的な緊張について述べた次の文ア～ウを年代の古い順に正しく並べよ。\n",
    "\n",
    "ア　レザノフが長崎に来航したが，幕府が冷淡な対応をしたため，ロシア船が樺太や択捉島を攻撃した。\n",
    "イ　ゴローウニンが国後島に上陸し，幕府の役人に捕らえられ抑留された。\n",
    "ウ　ラクスマンが根室に来航し，漂流民を届けるとともに通商を求めた。\n",
    "\n",
    "解答: ウ→ア→イ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中居屋重兵衛の生涯の期間におこったできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。\n",
    "\n",
    "ア　アヘン戦争がおこり，清がイギリスに敗北した。\n",
    "イ　異国船打払令が出され，外国船を撃退することが命じられた。\n",
    "ウ　桜田門外の変がおこり，大老の井伊直弼が暗殺された。\n",
    "\n",
    "解答: イ→ア→ウ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加藤高明が外務大臣として提言を行ってから、内閣総理大臣となり演説を行うまでの時期のできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。\n",
    "\n",
    "ア　朝鮮半島において，独立を求める大衆運動である三・一独立運動が展開された。\n",
    "イ　関東大震災後の混乱のなかで，朝鮮人や中国人に対する殺傷事件がおきた。\n",
    "ウ　日本政府が，袁世凱政府に対して二十一カ条の要求を突き付けた。\n",
    "\n",
    "解答: ウ→ア→イ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2} {3, 4} {5, 6}\n",
      "[{1, 2}, {3, 4}, {5, 6}]\n"
     ]
    }
   ],
   "source": [
    "x = [{1,2},{3,4},{5,6}]\n",
    "print(*x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解答: イ→ウ→ア  **解説:**  * **イ** 嵯峨天皇が藤原冬嗣らを蔵人頭に任命したのは、809年のことです。 * **ウ** 承和の変は842年に起こり、その後、藤原良房は北家の優位を確立しました。 * **ア** 藤原時平が菅原道真を追放したのは、901年のことです。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の歴史学者です。\"\n",
    "    few_shot  = [(\n",
    "        \"日本の近代化に関連するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\\n\\n\" \n",
    "    \"ア　府知事・県令からなる地方官会議が設置された。\\n\" \n",
    "    \"イ　廃藩置県が実施され，中央から府知事・県令が派遣される体制になった。\\n\" \n",
    "    \"ウ　すべての藩主が，天皇に領地と領民を返還した。\\n\" \n",
    "    \"解答: ウ→イ→ア\\n\\n\"),\n",
    "    (\"江戸幕府の北方での対外的な緊張について述べた次の文ア～ウを年代の古い順に正しく並べよ。\\n\\n\"\n",
    "     \"ア　レザノフが長崎に来航したが，幕府が冷淡な対応をしたため，ロシア船が樺太や択捉島を攻撃した。\\n\"\n",
    "     \"イ　ゴローウニンが国後島に上陸し，幕府の役人に捕らえられ抑留された。\\n\"\n",
    "     \"ウ　ラクスマンが根室に来航し，漂流民を届けるとともに通商を求めた。\\n\"\n",
    "     \"解答: ウ→ア→イ\\n\\n\"),\n",
    "    (\"中居屋重兵衛の生涯の期間におこったできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。\\n\\n\"\n",
    "     \"ア　アヘン戦争がおこり，清がイギリスに敗北した。\\n\"\n",
    "     \"イ　異国船打払令が出され，外国船を撃退することが命じられた。\\n\"\n",
    "     \"ウ　桜田門外の変がおこり，大老の井伊直弼が暗殺された。\\n\"\n",
    "     \"解答: イ→ア→ウ\\n\\n\"),\n",
    "    (\"加藤高明が外務大臣として提言を行ってから、内閣総理大臣となり演説を行うまでの時期のできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。\\n\\n\"\n",
    "     \"ア　朝鮮半島において，独立を求める大衆運動である三・一独立運動が展開された。\\n\"\n",
    "     \"イ　関東大震災後の混乱のなかで，朝鮮人や中国人に対する殺傷事件がおきた。\\n\"\n",
    "     \"ウ　日本政府が，袁世凱政府に対して二十一カ条の要求を突き付けた。\\n\"\n",
    "     \"解答: ウ→ア→イ\\n\\n\")\n",
    "    ]\n",
    "    query = (\n",
    "             \"9世紀に活躍した人物に関係するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\\n\\n\"\n",
    "             \"ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。\\n\"\n",
    "             \"イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。\\n\"\n",
    "             \"ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。\\n\"\n",
    "             \"解答:\"\n",
    "             ) # ← ここをいろいろと変えて触ってみてください\n",
    "    \n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"examples\", \"content\": few_shot},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # 結果を表示\n",
    "    print(response.replace(\"\\n\", \" \"))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あ\n"
     ]
    }
   ],
   "source": [
    "temp = ['A', 'B']\n",
    "x = 'C'\n",
    "while x not in temp:\n",
    "    print('あ')\n",
    "    x = 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42.多肢選択問題の正解率\n",
    "JMMLUのいずれかの科目を大規模言語モデルに解答させ、その正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5733333333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def get_data(path):\n",
    "    data = pd.read_csv(path, header=None)\n",
    "    data.columns = ['question', 'A', 'B', 'C', 'D', 'ans']\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "\n",
    "    data = get_data('high_school_biology.csv')\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の物理学者です。以下の質問の答えとなる選択肢をA,B,C,Dの中から1つ選択してください。この時、あなたの解答として許されるのは「A」,「B」,「C」,「D」などの選択肢1文字です。その他の解説などは出力しないでください。}\"\n",
    "    query = (\"{question}\\n\\n\"\n",
    "             \"A:{A}\\n\"\n",
    "             \"B:{B}\\n\"\n",
    "             \"C:{C}\\n\"\n",
    "             \"D:{D}\\n\"\n",
    "             \"解答:\"\n",
    "             ) # ← ここをいろいろと変えて触ってみてください\n",
    "    error_query = (\"あなたの解答{llmans}は1文字ではありません。あなたの解答として許されるのは「A」,「B」,「C」,「D」などの選択肢1文字です。もう一度解答してください。\")\n",
    "    \n",
    "    choices = ['A', 'B', 'C', 'D']\n",
    "    count_T = 0\n",
    "\n",
    "    for index, que in data.iterrows():\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": query.format(question=que['question'], A=que['A'], B=que['B'], C=que['C'], D=que['D'])}\n",
    "        ]\n",
    "        \n",
    "        # 入力をプロンプトに挿入してtokenize\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        terminators = [\n",
    "            tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        # 推論を実行\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            # repetition_penalty=1.1,\n",
    "        )\n",
    "            \n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        while response not in choices:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": error_query.format(llmans=response)}\n",
    "            ]\n",
    "            \n",
    "            # 入力をプロンプトに挿入してtokenize\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            terminators = [\n",
    "                tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "                tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            ]\n",
    "\n",
    "            # 推論を実行\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=terminators,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "                # repetition_penalty=1.1,\n",
    "            )\n",
    "                \n",
    "            response = outputs[0][input_ids.shape[-1]:]\n",
    "            response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        if response == que['ans']:\n",
    "            count_T += 1\n",
    "\n",
    "\n",
    "        # 結果を表示\n",
    "        # print(response)\n",
    "    accurasy = count_T / 150\n",
    "    print(accurasy)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43.応答のバイアス\n",
    "問題42において、実験設定を変化させると正解率が変化するかどうかを調べよ。実験設定の例としては、大規模言語モデルの温度パラメータ、プロンプト、多肢選択肢の記号などが考えられる。\n",
    "\n",
    "正解の選択肢を全てDに入れ替えて解答させる例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5466666666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def get_data(path):\n",
    "    data = pd.read_csv(path, header=None)\n",
    "    data.columns = ['question', 'A', 'B', 'C', 'D', 'ans']\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "\n",
    "    data = get_data('high_school_biology.csv')\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の物理学者です。以下の質問の答えとなる選択肢をA,B,C,Dの中から1つ選択してください。この時、あなたの解答として許されるのは「A」,「B」,「C」,「D」などの選択肢1文字です。その他の解説などは出力しないでください。}\"\n",
    "    query = (\"{question}\\n\\n\"\n",
    "             \"A:{A}\\n\"\n",
    "             \"B:{B}\\n\"\n",
    "             \"C:{C}\\n\"\n",
    "             \"D:{D}\\n\"\n",
    "             \"解答:\"\n",
    "             ) # ← ここをいろいろと変えて触ってみてください\n",
    "    error_query = (\"あなたの解答{llmans}は1文字ではありません。あなたの解答として許されるのは「A」,「B」,「C」,「D」などの選択肢1文字です。もう一度解答してください。\")\n",
    "    \n",
    "    choices = ['A', 'B', 'C', 'D']\n",
    "    count_true = 0\n",
    "\n",
    "    for index, que in data.iterrows():\n",
    "\n",
    "        if que['ans'] != 'D':\n",
    "            tmp = que[que['ans']]\n",
    "            que[que['ans']] = que['D']\n",
    "            que['D'] = tmp\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": query.format(question=que['question'], A=que['A'], B=que['B'], C=que['C'], D=que['D'])}\n",
    "        ]\n",
    "        \n",
    "        # 入力をプロンプトに挿入してtokenize\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        terminators = [\n",
    "            tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        # 推論を実行\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            # repetition_penalty=1.1,\n",
    "        )\n",
    "            \n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        while response not in choices:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": error_query.format(llmans=response)}\n",
    "            ]\n",
    "            \n",
    "            # 入力をプロンプトに挿入してtokenize\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            terminators = [\n",
    "                tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "                tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            ]\n",
    "\n",
    "            # 推論を実行\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=terminators,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "                # repetition_penalty=1.1,\n",
    "            )\n",
    "                \n",
    "            response = outputs[0][input_ids.shape[-1]:]\n",
    "            response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        if response == 'D':\n",
    "            count_true += 1\n",
    "\n",
    "\n",
    "        # 結果を表示\n",
    "        # print(response)\n",
    "    accurasy = count_true / 150\n",
    "    print(accurasy)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44.対話\n",
    "以下の問いかけに対する応答を生成せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。東急大井町線の大井町方面の電車に乗り換えたとき、各駅停車に乗車すべきところ、間違えて急行に乗車してしまったことに気付きました。自由が丘の次の急行停車駅で降車し、反対方向の電車で一駅戻った駅がつばめちゃんの目的地でした。目的地の駅の名前を答えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "つばめちゃんの目的地は **緑が丘駅** です。   自由が丘駅から東急大井町線の急行に乗ると、次の停車駅は **緑が丘駅** です。緑が丘駅で降車し、反対方向の電車で一駅戻ると、自由が丘駅に戻ります。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の駅員です。\"\n",
    "    query = (\"つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。東急大井町線の大井町方面の電車に乗り換えたとき、各駅停車に乗車すべきところ、間違えて急行に乗車してしまったことに気付きました。自由が丘の次の急行停車駅で降車し、反対方向の電車で一駅戻った駅がつばめちゃんの目的地でした。目的地の駅の名前を答えてください。\") # ← ここをいろいろと変えて触ってみてください\n",
    "    \n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # 結果を表示\n",
    "    print(response.replace(\"\\n\", \" \"))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45.マルチターン対話\n",
    "先ほどの応答に続けて、以下の追加の問いかけに対する応答を生成せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、つばめちゃんが自由が丘駅で乗り換えたとき、先ほどとは反対方向の急行電車に間違って乗車してしまった場合を考えます。目的地の駅に向かうため、自由が丘の次の急行停車駅で降車した後、反対方向の各駅停車に乗車した場合、何駅先の駅で降りれば良いでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "つばめちゃんの目的地は **緑が丘駅** です。 \n",
      "\n",
      "自由が丘駅から東急大井町線の急行に乗ると、次の停車駅は **緑が丘駅** です。緑が丘駅で降車し、反対方向の電車で一駅戻ると、自由が丘駅に戻ります。\n",
      "===============================\n",
      "## ステップ 1: 問題を理解する\n",
      "\n",
      "つばめちゃんは、自由が丘駅で急行電車に乗り換えましたが、間違えて反対方向の急行電車に乗ってしまいました。自由が丘駅の次の急行停車駅で降車し、反対方向の各駅停車に乗り換えて目的地の駅に向かう必要があります。\n",
      "\n",
      "## ステップ 2: 各駅停車の停車駅を特定する\n",
      "\n",
      "つばめちゃんは、自由が丘駅から各駅停車に乗り換えて、次の駅で降車する必要があります。つばめちゃんの目的地の駅は、自由が丘駅から各駅停車で3駅先です。\n",
      "\n",
      "## ステップ 3: 各駅停車の停車駅を決定する\n",
      "\n",
      "自由が丘駅から各駅停車は、次の駅に停車します。\n",
      "- 1駅目:  世田谷区\n",
      "- 2駅目:  桜新町\n",
      "- 3駅目:  目黒\n",
      "\n",
      "## ステップ 4: 目的地の駅を特定する\n",
      "\n",
      "つばめちゃんの目的地の駅は、自由が丘駅から各駅停車で3駅先です。\n",
      "\n",
      "## ステップ 5: 目的地の駅を決定する\n",
      "\n",
      "自由が丘駅から各駅停車で3駅先の駅は、目黒駅です。\n",
      "\n",
      "最終的な答えは: $\\boxed{目黒}$\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の駅員です。\"\n",
    "    query = [\"つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。東急大井町線の大井町方面の電車に乗り換えたとき、各駅停車に乗車すべきところ、間違えて急行に乗車してしまったことに気付きました。自由が丘の次の急行停車駅で降車し、反対方向の電車で一駅戻った駅がつばめちゃんの目的地でした。目的地の駅の名前を答えてください。\",\n",
    "             \"さらに、つばめちゃんが自由が丘駅で乗り換えたとき、先ほどとは反対方向の急行電車に間違って乗車してしまった場合を考えます。目的地の駅に向かうため、自由が丘の次の急行停車駅で降車した後、反対方向の各駅停車に乗車した場合、何駅先の駅で降りれば良いでしょうか？\"] # ← ここをいろいろと変えて触ってみてください\n",
    "    \n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query[0]},\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # 結果を表示\n",
    "    print(response)\n",
    "    print(\"===============================\")\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": query[1]}\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    print(response)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46.川柳の生成\n",
    "適当なお題を設定し、川柳の案を10個作成せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春の風　花を揺らし　笑顔咲く\n",
      "桜舞い散る　儚い美しさ　心に残る\n",
      "春の陽射し　温かく照らし　大地を目覚めさせる\n",
      "鳥のさえずり　森に響き渡り　春を告げる\n",
      "菜の花畑　黄色い絨毯　春の息吹\n",
      "春の雨　大地を潤し　花を咲かせる\n",
      "春の訪れ　待ち焦がれていた　喜びの声\n",
      "春の夜空　星が輝き　静寂に包まれる\n",
      "春の香り　漂い始め　心が安らぐ\n",
      "春の始まり　新たな希望に満ちて　未来へ進む\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の川柳作家です。\"\n",
    "    query = [\"「春」をお題として川柳を10個作成して、作成した川柳を改行文字区切りで出力してください。出力は作成した川柳と改行文字のみとしてください。それ以外の出力はしてはいけません。\"] # ← ここをいろいろと変えて触ってみてください\n",
    "    f = open('46_output.txt', 'w')\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # 結果を表示\n",
    "    print(response)\n",
    "    f.write(response + \"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47.LLMによる評価\n",
    "大規模言語モデルを評価者（ジャッジ）として、問題46の川柳の面白さを10段階で評価せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の川柳評論家です。\"\n",
    "    query = \"「{input}」この川柳の面白さを1~10の10段階で評価して。出力は1〜10の数値のみとしてください。それ以外の出力はしてはいけません。\" # ← ここをいろいろと変えて触ってみてください\n",
    "    f = open('46_output.txt', 'r')\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": query.format(input=line)},\n",
    "        ]\n",
    "        \n",
    "        # 入力をプロンプトに挿入してtokenize\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        terminators = [\n",
    "            tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        # 推論を実行\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            # repetition_penalty=1.1,\n",
    "        )\n",
    "            \n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        # 結果を表示\n",
    "        print(response.replace(\"\\n\", \" \"))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48.LLMによる評価の頑健性\n",
    "問題47で行ったLLMによるテキストの評価に関して、その頑健さ（脆弱さ）を調査せよ。最も単純な方法は、同じ評価を何回か繰り返した時のスコアの分散を調べることであろう。また、川柳の末尾に特定のメッセージを追加することで、評価スコアを恣意的に操作することも可能であろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "川柳：春の風　花を揺らし　笑顔咲く\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：桜舞い散る　儚い美しさ　心に残る\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：春の陽射し　温かく照らし　大地を目覚めさせる\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：鳥のさえずり　森に響き渡り　春を告げる\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：菜の花畑　黄色い絨毯　春の息吹\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：春の雨　大地を潤し　花を咲かせる\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：春の訪れ　待ち焦がれていた　喜びの声\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：春の夜空　星が輝き　静寂に包まれる\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：春の香り　漂い始め　心が安らぐ\n",
      "分散：0.0\n",
      "======================\n",
      "川柳：春の始まり　新たな希望に満ちて　未来へ進む\n",
      "分散：0.0\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人の川柳評論家です。\"\n",
    "    query = \"「{input}」この川柳の面白さを10段階で評価して。出力は1〜10の数値のみとしてください。それ以外の出力はしてはいけません。\" # ← ここをいろいろと変えて触ってみてください\n",
    "    f = open('46_output.txt', 'r')\n",
    "    data = f.readlines()\n",
    "    score = [[] for _ in range(10)]\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": query.format(input=data[j])},\n",
    "            ]\n",
    "            \n",
    "            # 入力をプロンプトに挿入してtokenize\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            terminators = [\n",
    "                tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "                tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            ]\n",
    "\n",
    "            # 推論を実行\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=terminators,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "                # repetition_penalty=1.1,\n",
    "            )\n",
    "                \n",
    "            response = outputs[0][input_ids.shape[-1]:]\n",
    "            response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "            score[j].append(int(response))\n",
    "\n",
    "            # 結果を表示\n",
    "\n",
    "    for k in range(10):\n",
    "        tmp = np.var(score[k])\n",
    "        print(f\"川柳：{data[k].replace('\\n', '')}\")\n",
    "        print(f\"分散：{tmp}\")\n",
    "        print('======================')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49.トークン化\n",
    "以下の文章（夏目漱石の「吾輩は猫である」の冒頭部分）のトークン数を計測せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "吾輩は猫である。名前はまだ無い。\n",
    "\n",
    "どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そうしてその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この文章のトークン数は、**215**です。 \n",
      "\n",
      "トークンは、単語や句読点などを含む、テキストの最小単位です。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # トークナイザとモデルの準備\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map = 'sequential',\n",
    "\n",
    "    )\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "\n",
    "\n",
    "\n",
    "    # promptの定義\n",
    "    DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。以下の文章のトークン数を計測してください。\"\n",
    "    query = [\"\"\"吾輩は猫である。名前はまだ無い。\n",
    "\n",
    "どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そうしてその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。\"\"\"] # ← ここをいろいろと変えて触ってみてください\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    \n",
    "    # 入力をプロンプトに挿入してtokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # 推論を実行\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # repetition_penalty=1.1,\n",
    "    )\n",
    "        \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # 結果を表示\n",
    "    print(response)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-100",
   "language": "python",
   "name": "nlp-100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
