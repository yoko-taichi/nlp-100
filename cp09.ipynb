{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第9章：事前学習済み言語モデル（BERT型）\n",
    "本章では、BERT型の事前学習済みモデルを利用して、単語マスクの予測や文ベクトルの計算、評判分析器（ポジネガ分類機）の構築に取り組む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80.トークン化\n",
    "“The movie was full of incomprehensibilities.”という文をトークンに分解し、トークン列を表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = 'The movie was full of incomprehensibilities.'\n",
    "token = tokenizer.tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81.マスクの予測\n",
    "“The movie was full of [MASK].”の”[MASK]”に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.10711904615163803, 'token': 4569, 'token_str': 'fun', 'sequence': 'the movie was full of fun.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model_name, top_k=1)\n",
    "text = 'The movie was full of [MASK].'\n",
    "result = unmasker(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82.マスクのtop-k予測\n",
    "“The movie was full of [MASK].”の”[MASK]”に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:fun score:0.10711904615163803\n",
      "token:surprises score:0.06634485721588135\n",
      "token:drama score:0.04468414932489395\n",
      "token:stars score:0.027217062190175056\n",
      "token:laughs score:0.025412775576114655\n",
      "token:action score:0.01951693743467331\n",
      "token:excitement score:0.019038118422031403\n",
      "token:people score:0.018290281295776367\n",
      "token:tension score:0.015030575916171074\n",
      "token:music score:0.014646227471530437\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model_name, top_k=10)\n",
    "result = unmasker(text)\n",
    "\n",
    "for mask in result:\n",
    "    print(f'token:{mask['token_str']} score:{mask['score']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83.CLSトークンによる文ベクトル\n",
    "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
    "\n",
    "・“The movie was full of fun.”\n",
    "\n",
    "・“The movie was full of excitement.”\n",
    "\n",
    "・“The movie was full of crap.”\n",
    "\n",
    "・“The movie was full of rubbish.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1386601/2039098937.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(tokenize_text['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999  0.98806083 0.955766   0.94753236]\n",
      " [0.98806083 1.         0.9541273  0.9486635 ]\n",
      " [0.955766   0.9541273  0.99999976 0.9806931 ]\n",
      " [0.94753236 0.9486635  0.9806931  1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sentences = [\n",
    "    'The movie was full of fun.',\n",
    "    'The movie was full of excitement.',\n",
    "    'The movie was full of crap.',\n",
    "    'The movie was full of rubbish.'\n",
    "]\n",
    "\n",
    "tokenize_text = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input = torch.tensor(tokenize_text['input_ids'])\n",
    "outputs = model(input)\n",
    "\n",
    "last_hidden_states = outputs[0]\n",
    "sentencevec = last_hidden_states[:,0,:].detach().cpu().numpy()\n",
    "\n",
    "cos = cosine_similarity(sentencevec)\n",
    "print(cos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84.平均による文ベクトル\n",
    "以下の文全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
    "\n",
    "・“The movie was full of fun.”\n",
    "\n",
    "・“The movie was full of excitement.”\n",
    "\n",
    "・“The movie was full of crap.”\n",
    "\n",
    "・“The movie was full of rubbish.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1386601/1868542933.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(tokenize_text['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000001  0.95681167 0.8489994  0.8168843 ]\n",
      " [0.95681167 0.9999999  0.83518374 0.7938444 ]\n",
      " [0.8489994  0.83518374 0.9999999  0.92255414]\n",
      " [0.8168843  0.7938444  0.92255414 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'The movie was full of fun.',\n",
    "    'The movie was full of excitement.',\n",
    "    'The movie was full of crap.',\n",
    "    'The movie was full of rubbish.'\n",
    "]\n",
    "\n",
    "tokenize_text = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input = torch.tensor(tokenize_text['input_ids'])\n",
    "outputs = model(input)\n",
    "\n",
    "last_hidden_states = outputs[0]\n",
    "sentencevec = last_hidden_states.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "cos = cosine_similarity(sentencevec)\n",
    "print(cos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85.データセットの準備\n",
    "General Language Understanding Evaluation (GLUE) ベンチマークで配布されているStanford Sentiment Treebank (SST) から訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'hide new secretions from the parental units ', 'label': 0, 'token': ['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units']}\n",
      "{'text': 'contains no wit , only labored gags ', 'label': 0, 'token': ['contains', 'no', 'wit', ',', 'only', 'labor', '##ed', 'gag', '##s']}\n",
      "{'text': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': 1, 'token': ['that', 'loves', 'its', 'characters', 'and', 'communicate', '##s', 'something', 'rather', 'beautiful', 'about', 'human', 'nature']}\n",
      "{'text': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'token': ['remains', 'utterly', 'satisfied', 'to', 'remain', 'the', 'same', 'throughout']}\n",
      "{'text': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': 0, 'token': ['on', 'the', 'worst', 'revenge', '-', 'of', '-', 'the', '-', 'ne', '##rds', 'cl', '##iche', '##s', 'the', 'filmmakers', 'could', 'dr', '##edge', 'up']}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_token(df):\n",
    "    result = []\n",
    "    for i, item in df.iterrows():\n",
    "        label = item['label']\n",
    "        text = item['sentence']\n",
    "        token = tokenizer.tokenize(text)\n",
    "\n",
    "        result.append({'text': text, 'label': label, 'token': token})\n",
    "    return result\n",
    "\n",
    "df_train = pd.read_csv('cp07-data/SST-2/train.tsv', sep='\\t')\n",
    "df_dev = pd.read_csv('cp07-data/SST-2/dev.tsv', sep='\\t')\n",
    "\n",
    "data_train = make_token(df_train)\n",
    "data_dev = make_token(df_dev)\n",
    "\n",
    "for i in range(5):\n",
    "    print(data_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86.ミニバッチの作成\n",
    "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'hide new secretions from the parental units ', 'label': 0, 'input_ids': {'input_ids': tensor([[  101,  5342,  2047,  3595,  8496,  2013,  1996, 18643,  3197,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_token(df):\n",
    "    result = []\n",
    "    for i, item in df.iterrows():\n",
    "        label = item['label']\n",
    "        text = item['sentence']\n",
    "        token = tokenizer(text, return_tensors=\"pt\", max_length=128, padding=\"max_length\")\n",
    "\n",
    "        result.append({'text': text, 'label': label, 'input_ids': token})\n",
    "    return result\n",
    "\n",
    "df_train = pd.read_csv('cp07-data/SST-2/train.tsv', sep='\\t')\n",
    "df_dev = pd.read_csv('cp07-data/SST-2/dev.tsv', sep='\\t')\n",
    "\n",
    "data_train = make_token(df_train)\n",
    "data_dev = make_token(df_dev)\n",
    "\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.0\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87.ファインチューニング\n",
    "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:19<00:00, 3512.06 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 3011.59 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5265' max='5265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5265/5265 21:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>0.919725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.256958</td>\n",
       "      <td>0.909404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.288396</td>\n",
       "      <td>0.926606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.328246</td>\n",
       "      <td>0.931193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.496765</td>\n",
       "      <td>0.924312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5265, training_loss=0.12310158927895744, metrics={'train_runtime': 1298.3701, 'train_samples_per_second': 259.36, 'train_steps_per_second': 4.055, 'total_flos': 7721667113841000.0, 'train_loss': 0.12310158927895744, 'epoch': 5.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BatchEncoding, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def make_dataset(file_name):\n",
    "  df = pd.read_csv(file_name, sep='\\t')\n",
    "  df['label'] = df['label'].astype(int)\n",
    "  df['tokens'] = df['sentence'].apply(tokenizer.tokenize)\n",
    "  return Dataset.from_pandas(df)\n",
    "\n",
    "def compute_accuracy(eval_pred: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "  predictions, labels = eval_pred\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "train_dataset =  make_dataset('cp07-data/SST-2/train.tsv')\n",
    "dev_dataset = make_dataset('cp07-data/SST-2/dev.tsv')\n",
    "\n",
    "def preprocess_text_classification(\n",
    "        example: dict[str, str | int]\n",
    ") -> BatchEncoding:\n",
    "    \n",
    "    encoded_example = tokenizer(example['sentence'], max_length=128)\n",
    "    encoded_example['labels'] = example['label']\n",
    "    return encoded_example\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_text_classification, remove_columns=train_dataset.column_names)\n",
    "encoded_dev_dataset = dev_dataset.map(preprocess_text_classification, remove_columns=dev_dataset.column_names)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class_label = train_dataset.features['label']\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/yokoyama/nlp-100/models/cp09/',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88.極性分析\n",
    "問題87でファインチューニングされたモデルを用いて、以下の分の極性を予測せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1386601/813749455.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(token['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of fun.  0\n",
      "The movie was full of excitement.  0\n",
      "The movie was full of crap.  0\n",
      "The movie was full of rubbish.  0\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'The movie was full of fun.',\n",
    "    'The movie was full of excitement.',\n",
    "    'The movie was full of crap.',\n",
    "    'The movie was full of rubbish.'\n",
    "]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/home/yokoyama/nlp-100/models/cp09/checkpoint-5265')\n",
    "token = tokenizer(sentences, return_tensors=\"pt\", max_length=128, padding=\"max_length\")\n",
    "\n",
    "input_ids = torch.tensor(token['input_ids'])\n",
    "output = model(input_ids)\n",
    "\n",
    "for sentens, output in zip(sentences, output.logits):\n",
    "    print(f'{sentens}  {np.argmax(output.detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89.アーキテクチャの変更\n",
    "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Map: 100%|██████████| 67349/67349 [00:16<00:00, 4032.03 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 3409.91 examples/s]\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5265' max='5265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5265/5265 21:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.289200</td>\n",
       "      <td>0.220065</td>\n",
       "      <td>0.920872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.242296</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.280345</td>\n",
       "      <td>0.925459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.328229</td>\n",
       "      <td>0.930046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.523736</td>\n",
       "      <td>0.924312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas8/data/home/yokoyama/nlp-100/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5265, training_loss=0.12215669854753718, metrics={'train_runtime': 1307.5842, 'train_samples_per_second': 257.532, 'train_steps_per_second': 4.027, 'total_flos': 0.0, 'train_loss': 0.12215669854753718, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, BatchEncoding, DataCollatorWithPadding, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def make_dataset(file_name):\n",
    "  df = pd.read_csv(file_name, sep='\\t')\n",
    "  df['label'] = df['label'].astype(int)\n",
    "  df['tokens'] = df['sentence'].apply(tokenizer.tokenize)\n",
    "  return Dataset.from_pandas(df)\n",
    "\n",
    "def compute_accuracy(eval_pred: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "  predictions, labels = eval_pred\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "train_dataset =  make_dataset('cp07-data/SST-2/train.tsv')\n",
    "dev_dataset = make_dataset('cp07-data/SST-2/dev.tsv')\n",
    "\n",
    "def preprocess_text_classification(\n",
    "        example: dict[str, str | int]\n",
    ") -> BatchEncoding:\n",
    "    \n",
    "    encoded_example = tokenizer(example['sentence'], max_length=128)\n",
    "    encoded_example['labels'] = example['label']\n",
    "    return encoded_example\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_text_classification, remove_columns=train_dataset.column_names)\n",
    "encoded_dev_dataset = dev_dataset.map(preprocess_text_classification, remove_columns=dev_dataset.column_names)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class_label = train_dataset.features['label']\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.regressor = nn.Linear(self.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        hidden_states = hidden_states.masked_fill(mask_expanded == 0, -1e9)\n",
    "        pooled_output, _ = torch.max(hidden_states, dim=1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.num_labels == 1:\n",
    "                # 回帰問題 (CommonLitなど)\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                # 分類問題 (例えばSST-2など)\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )  # max pooling\n",
    "    \n",
    "model = CommonLitModel(model_name=model_name, num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/yokoyama/nlp-100/models/cp09/',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-100",
   "language": "python",
   "name": "nlp-100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
